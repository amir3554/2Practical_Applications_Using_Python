this unit talks about practical uses for python language , at first a learned about strings and the operations that is used for (processing the sting, indexing , slicing, letters cases, splitting, deleting, adding and searching and more operations).

And nearly the same operations for the lists, tuples and dictionaries (if element in []...,append(), pop(), get(), map(func, iterable), filter(func, iterable),copy(), update() and more ...).

using my skills and knowledge of these , i have made a tic-tac-toe game in python.

In the 4th unit , i have learned how to use the regular expressions using pythex, (it is a sequence of characters that specifies a match pattern in text.) (re.match(pattern, string), re.search(pattern, string), re.findall(pattern, string), re.sub(pattern, repl, string) (. ^ $ * + ? [] | ).

Unit 5 talked about dealing with files on the system (using the os library) (folders, text files , paths, zip files), doing some operations on them (make, open, read, write, delete, copy, rename and change place and path) ...

After that i made operations on excel files ; open the file and read the date row by row , cell by cell, write data and make mathematic operations like (sum, max, min, average ... ), excel charts and finally make a multiplication table py app which will take the number you have inserted and make an excel table for you.



As well as Working with Google Sheets and making various operations on them like the above ,and also operations on PDF and Word files .


One of the important units is the one that is talking about dealing and processing JSON and CSV files Which are used for storing data , especially JSON (JavaScript Object Notation) files, nearly every web application uses them to exchange data across the APIs and between the client and the server .

This repository(9 Web Scrapping) includes a short, step-by-step guide to web scraping using Python. I began by learning the concept of web scraping and how to programmatically open a Google Maps site. I then learned how to parse HTML pages and extract data from them, whether from the web or from local HTML files. I then moved on to configuring Chrome and Firefox browsers using the Selenium module, then using Selenium to extract data from web pages and control the browser automatically. Finally, I compiled the extracted data and store it in a CSV file, ready for use in your future analyses or projects.

In The 10PythonDB i have learned about extract data from (CSV files to DataBase), form (DataBase To excel file), making database and writing some SQL commands (Inserting values, deleting, adding) at the end of the unit i managed to make a "to-do" app where you can add your tasks and their info like title, the_info(description) , date, whether they are done or no (Boolean), add new task, edit, check, delete show all, save DB, based on sys CLI commands.



Finally i have learned a lot of practical applications form the last unit: working with Gmail programmatically using Python: from enabling the Gmail API and configuring all necessary settings, to sending and reading messages via the Gmail API, connecting to an SMTP server to send mail, connecting to an IMAP server to retrieve and view messages, and even how to automatically send member dues reminders. The example relies on official libraries such as google-api-python-client and oauth2client to communicate with the Gmail API, and smtplib and imaplib to manage traditional sending and receiving.




"hasoub academy units for python (unit 2)"